{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 모델 1: snunlp/KR-ELECTRA-discriminator 테스트 데이터 예측 시작 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 35/35 [00:09<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 모델 2: beomi/KcELECTRA-base-v2022 테스트 데이터 예측 시작 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 35/35 [00:09<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 모델 3: monologg/koelectra-base-v3-discriminator 테스트 데이터 예측 시작 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 35/35 [00:02<00:00, 12.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# test.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# STSDataset 클래스는 앞서 수정한 내용을 사용합니다.\n",
    "class STSDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, is_train=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.is_train = is_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence1 = str(self.df.loc[idx, 'sentence_1'])\n",
    "        sentence2 = str(self.df.loc[idx, 'sentence_2'])\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence1,\n",
    "            sentence2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze()  # [max_len]\n",
    "        attention_mask = inputs['attention_mask'].squeeze()  # [max_len]\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': input_ids.to(device),\n",
    "            'attention_mask': attention_mask.to(device),\n",
    "        }\n",
    "        \n",
    "        if self.is_train:\n",
    "            label = self.df.loc[idx, 'label']\n",
    "            item['labels'] = torch.tensor(label, dtype=torch.float).to(device)\n",
    "        \n",
    "        return item\n",
    "# 모델 클래스 정의 (이전과 동일)\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.fc1 = nn.Linear(self.config.hidden_size, self.config.hidden_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.regressor = nn.Linear(self.config.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            pooled_output = outputs.pooler_output\n",
    "        else:\n",
    "            pooled_output = outputs.last_hidden_state[:, 0]\n",
    "        \n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        outputs = self.regressor(x)\n",
    "        return outputs.squeeze()\n",
    "\n",
    "\n",
    "# 테스트 데이터 로드\n",
    "test_df = pd.read_csv('/data/ephemeral/home/data/test.csv')  # 테스트 데이터 파일 경로를 입력하세요\n",
    "\n",
    "# 토크나이저와 데이터셋 생성\n",
    "max_len = 128\n",
    "batch_size = 32\n",
    "\n",
    "model_names = [\n",
    "    'snunlp/KR-ELECTRA-discriminator',\n",
    "    'beomi/KcELECTRA-base-v2022',\n",
    "    'monologg/koelectra-base-v3-discriminator',\n",
    "]\n",
    "\n",
    "tokenizers = []\n",
    "test_loaders = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizers.append(tokenizer)\n",
    "    \n",
    "    test_dataset = STSDataset(test_df, tokenizer, max_len, is_train=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    test_loaders.append(test_loader)\n",
    "\n",
    "# 테스트 데이터에 대한 예측 수행\n",
    "test_predictions = []\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f\"\\n=== 모델 {i+1}: {model_name} 테스트 데이터 예측 시작 ===\")\n",
    "    # 모델 생성 및 체크포인트 로드\n",
    "    model = RegressionModel(model_name).to(device)\n",
    "\n",
    "    # checkpoint = torch.load(f'/data/ephemeral/home/yujin/results/model_{i+1}.pt')\n",
    "    # # 체크포인트에서 현재 모델과 크기가 일치하는 파라미터만 필터링\n",
    "    # filtered_state_dict = {k: v for k, v in checkpoint.items() if k in model.state_dict() and model.state_dict()[k].shape == v.shape}\n",
    "    # # 모델에 로드\n",
    "    # model.load_state_dict(filtered_state_dict, strict=False)\n",
    "    # try:\n",
    "    #     # State_dict에서 불일치하는 파라미터를 건너뛰고 로드\n",
    "    #     model.load_state_dict(filtered_state_dict, strict=False)\n",
    "    # except RuntimeError as e:\n",
    "    #     print(f\"RuntimeError occurred: {e}\")\n",
    "    #     print(\"Continuing with the rest of the code...\")\n",
    "    \n",
    "    model.load_state_dict(torch.load(f'/data/ephemeral/home/yujin/results/new_model_{i+1}.pt'), strict=False)\n",
    "    model.eval()\n",
    "    \n",
    "    test_loader = test_loaders[i]\n",
    "    \n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Testing'):\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask']\n",
    "            )\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    test_predictions.append(preds)\n",
    "\n",
    "# 앙상블 예측 (테스트 데이터)\n",
    "ensemble_test_preds = np.mean(test_predictions, axis=0)\n",
    "# 예측 값을 필요한 범위로 클리핑 (예: 0~5 사이)\n",
    "ensemble_test_preds = np.clip(ensemble_test_preds, 0.0, 5.0)\n",
    "\n",
    "ensemble_test_preds = np.round(ensemble_test_preds,1)\n",
    "\n",
    "# 결과 저장\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': ensemble_test_preds\n",
    "})\n",
    "\n",
    "# CSV 파일로 저장\n",
    "output_df.to_csv('/data/ephemeral/home/yujin/results/output_02.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메타 모델 학습을 완료하였습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 테스트 데이터 예측 시작 ===\n",
      "\n",
      "--- 모델 1: kykim/bert-kor-base ---\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/ephemeral/home/yujin/model_checkpoints/model_fold1_1.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 133\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# 모델 생성 및 체크포인트 로드\u001b[39;00m\n\u001b[1;32m    132\u001b[0m model \u001b[38;5;241m=\u001b[39m RegressionModel(model_name)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 133\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/ephemeral/home/yujin/model_checkpoints/model_fold\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    134\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    136\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m test_loaders[i]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/ephemeral/home/yujin/model_checkpoints/model_fold1_1.pt'"
     ]
    }
   ]
