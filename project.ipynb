{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-24 01:57:28.078681: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-24 01:57:28.084570: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-24 01:57:28.098561: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-24 01:57:28.121823: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-24 01:57:28.129071: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-24 01:57:28.147407: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-24 01:57:29.194282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import torchmetrics\n",
    "from scipy.stats import pearsonr\n",
    "import evaluate\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "random.seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, tokenizer, max_length, mode = 'train'):\n",
    "        self.inputs = inputs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.text_columns = ['sentence_1', 'sentence_2']\n",
    "        self.mode = mode\n",
    "    def cleaning_text(self, text):\n",
    "        cleaned_text = re.sub(r'[ㅋ-ㅎ]+', '', text)\n",
    "        cleaned_text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        t = self.inputs.iloc[idx] \n",
    "\n",
    "        text = '[SEP]'.join([t[col] for col in self.text_columns])\n",
    "        text = self.cleaning_text(text)\n",
    "        output = self.tokenizer(text,\n",
    "                                padding='max_length',\n",
    "                                max_length=self.max_length,\n",
    "                                truncation=True)\n",
    "\n",
    "        datas = torch.tensor(output['input_ids'], dtype = torch.long)\n",
    "        attn = torch.tensor(output['attention_mask'], dtype = torch.long)\n",
    "        type_ids = torch.tensor(output['token_type_ids'], dtype = torch.long)\n",
    "        if self.mode == 'train':\n",
    "            labels = t['label']\n",
    "            output = {'input_ids' : datas,\n",
    "                      'attention_mask' : attn,\n",
    "                      'token_type_ids' : type_ids,\n",
    "                      'labels' : labels}\n",
    "            return output\n",
    "        else:\n",
    "            output = {'input_ids' : datas,\n",
    "                      'attention_mask' : attn,\n",
    "                      'token_type_ids' : type_ids}\n",
    "            return output\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model_preds):\n",
    "    preds, labels = model_preds\n",
    "    preds = torch.tensor(preds, dtype = torch.float32).squeeze(-1)\n",
    "    labels = torch.tensor(labels, dtype = torch.float32).squeeze(-1)\n",
    "    pear = torchmetrics.PearsonCorrCoef()\n",
    "    pearson = pear(preds, labels)\n",
    "    return {'pearson' : pearson.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = transformers.AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # 첫 번째 Conv1D 레이어\n",
    "        self.Conv1 = nn.Conv1d(\n",
    "            in_channels=768,  # BERT의 출력 차원\n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        # 두 번째 Conv1D 레이어 (필요 시 추가)\n",
    "        self.Conv2 = nn.Conv1d(\n",
    "            in_channels=256,  # Conv1의 출력 차원\n",
    "            out_channels=128,  # Conv2의 출력 차원\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(128, 1)  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)  \n",
    "        self.batchnorm1 = nn.BatchNorm1d(256)  \n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)  \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size = 2)  \n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels = None):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        output = output.last_hidden_state.permute(0, 2, 1)  \n",
    "\n",
    "        cnn_output = self.Conv1(output)  # Shape: (B, 256, L)\n",
    "        cnn_output = self.relu(cnn_output)\n",
    "        cnn_output = self.batchnorm1(cnn_output)  \n",
    "        cnn_output = self.maxpool(cnn_output) # (B, 128, L/2)\n",
    "\n",
    "        cnn_output = self.Conv2(cnn_output)  # Shape: (B, 128, L/2)\n",
    "        cnn_output = self.relu(cnn_output)\n",
    "        cnn_output = self.batchnorm2(cnn_output) \n",
    "        cnn_output = self.avg_pool(cnn_output)  #(B, 128, 1)\n",
    "\n",
    "        cnn_output = cnn_output.view(cnn_output.size(0), -1)  # Shape: (B, 128)\n",
    "        output = self.output_layer(cnn_output).squeeze(-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(output, labels.float())\n",
    "            return {'output' : output, 'loss' : loss}\n",
    "\n",
    "        else:  \n",
    "            return {'output' : output}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def maketrain(args,training_args):\n",
    "\n",
    "    model_list = args.model_list\n",
    "    max_length = args.max_length\n",
    "    k = args.kf\n",
    "    kf = KFold(n_splits = k, shuffle = True, random_state = 0)\n",
    "    data_routes = args.data_routes\n",
    "    preds = {}\n",
    "    test = pd.read_csv('/data/ephemeral/home/data/test.csv')\n",
    "    df = pd.DataFrame()\n",
    "    for route in data_routes:\n",
    "        df = pd.concat([df, pd.read_csv(route)])\n",
    "    df.reset_index(drop = True)\n",
    "    df = df[['sentence_1', 'sentence_2' ,'label']].dropna()\n",
    "\n",
    "\n",
    "\n",
    "    for model_name in model_list:\n",
    "        name = model_name.split('/')[-1]\n",
    "        model = MyModel(model_name)\n",
    "        training_args.output_dir = f\"./results/{name}\"\n",
    "        training_args.run_name = f'{name}'\n",
    "        wandb_run = wandb.init(project = \"yongruka\", name = f\"{name}\", reinit = True)\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, trust_remote_code = True)\n",
    "        test_dataset = Dataset(test, tokenizer, max_length, mode = 'test')\n",
    "        data_collator = DataCollatorWithPadding(\n",
    "                tokenizer = tokenizer,\n",
    "                padding = True,\n",
    "                return_tensors = 'pt'\n",
    "            )\n",
    "        \n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(df)):\n",
    "\n",
    "            print(f'-Fold : {fold+1}-  /  Now_model : [{name}]')\n",
    "\n",
    "            train_fold = df.iloc[train_index]\n",
    "            val_fold = df.iloc[val_index]\n",
    "            train_fold = Dataset(train_fold, tokenizer, max_length)\n",
    "            val_fold = Dataset(val_fold, tokenizer, max_length)\n",
    "\n",
    "            trainer = Trainer( \n",
    "                model = model,\n",
    "                tokenizer = tokenizer,\n",
    "                args = training_args,\n",
    "                train_dataset = train_fold,\n",
    "                eval_dataset = val_fold,\n",
    "                compute_metrics = compute_metrics,\n",
    "                data_collator = data_collator,\n",
    "                \n",
    "            )\n",
    "\n",
    "        \n",
    "            trainer.train()\n",
    "        trainer.save_model(f'results/best_model_{name}')\n",
    "\n",
    "        pred = trainer.predict(test_dataset)\n",
    "        preds[name] = pred\n",
    "        gc.collect()\n",
    "        \n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkimybjg2\u001b[0m (\u001b[33mkimybjg2-boostcampaitech\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/moruka/wandb/run-20240924_015735-gx63arlp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka/runs/gx63arlp' target=\"_blank\">hopeful-valley-57</a></strong> to <a href='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Yonruka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka/runs/gx63arlp' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Yonruka/runs/gx63arlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka/runs/gx63arlp?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f7d70bf5870>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!rm -rf /root/.cache/wandb\n",
    "!rm -rf /root/.config/wandb\n",
    "!rm -rf /root/.netrc\n",
    "os.environ[\"WANDB_API_KEY\"] = \"ea26fff0d932bc74bbfad9fd507b292c67444c02\"\n",
    "wandb.init(project=\"yonruka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [#'klue/roberta-small',\n",
    "            'klue/roberta-base',\n",
    "            #'snunlp/KR-SBERT-Medium-extended-klueNLItriplet_PARpair_QApair-klueSTS',\n",
    "            'snunlp/KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS',\n",
    "            'klue/bert-base',\n",
    "            #'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "            #  'Alibaba-NLP/gte-multilingual-base',\n",
    "            ]\n",
    "data_routes = ['/data/ephemeral/home/data/aug_data40000.csv',]\n",
    "            #    '/data/ephemeral/home/data/dev.csv']\n",
    "            #    '/data/ephemeral/home/data/aug50000.csv']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_list', default = model_list, type = list)\n",
    "parser.add_argument('--batch_size', default = 64, type = int)\n",
    "parser.add_argument('--max_epoch', default = 5, type = int)\n",
    "parser.add_argument('--max_length', default = 160, type = int)\n",
    "parser.add_argument('--kf', default = 5, type = int)\n",
    "parser.add_argument('--data_routes', default = data_routes, type = list)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f\"./results/default\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = 'epoch',\n",
    "    per_device_train_batch_size = args.batch_size,\n",
    "    per_device_eval_batch_size = args.batch_size,\n",
    "    num_train_epochs = args.max_epoch,\n",
    "    weight_decay = 0.01,\n",
    "    logging_dir = './logs',\n",
    "    logging_steps = 30,\n",
    "    report_to = \"wandb\",  \n",
    "    run_name = \"default\",\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'pearson'\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gx63arlp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hopeful-valley-57</strong> at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka/runs/gx63arlp' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Yonruka/runs/gx63arlp</a><br/> View project at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Yonruka</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240924_015735-gx63arlp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gx63arlp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/moruka/wandb/run-20240924_015756-rj2u9e7x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka/runs/rj2u9e7x' target=\"_blank\">roberta-base</a></strong> to <a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/yongruka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka/runs/rj2u9e7x' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/yongruka/runs/rj2u9e7x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Fold : 1-  /  Now_model : [roberta-base]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 24:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.079700</td>\n",
       "      <td>4.550874</td>\n",
       "      <td>0.954294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.258900</td>\n",
       "      <td>3.198948</td>\n",
       "      <td>0.975929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.164600</td>\n",
       "      <td>1.976188</td>\n",
       "      <td>0.982551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.652700</td>\n",
       "      <td>1.577736</td>\n",
       "      <td>0.986036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.481200</td>\n",
       "      <td>1.297059</td>\n",
       "      <td>0.987742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Fold : 2-  /  Now_model : [roberta-base]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 24:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>0.432266</td>\n",
       "      <td>0.986991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.134900</td>\n",
       "      <td>0.088220</td>\n",
       "      <td>0.989625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.093100</td>\n",
       "      <td>0.038649</td>\n",
       "      <td>0.993942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.028191</td>\n",
       "      <td>0.996004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.039477</td>\n",
       "      <td>0.996616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Fold : 3-  /  Now_model : [roberta-base]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 24:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.034834</td>\n",
       "      <td>0.993401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.031949</td>\n",
       "      <td>0.996327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.026679</td>\n",
       "      <td>0.997212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.024971</td>\n",
       "      <td>0.998152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.029893</td>\n",
       "      <td>0.998420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Fold : 4-  /  Now_model : [roberta-base]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 24:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.022902</td>\n",
       "      <td>0.996584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.033796</td>\n",
       "      <td>0.996909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.018009</td>\n",
       "      <td>0.998206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.023024</td>\n",
       "      <td>0.998565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.013652</td>\n",
       "      <td>0.998961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Fold : 5-  /  Now_model : [roberta-base]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 24:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.037926</td>\n",
       "      <td>0.996942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>0.018105</td>\n",
       "      <td>0.998457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.010242</td>\n",
       "      <td>0.998799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.022104</td>\n",
       "      <td>0.998999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.012575</td>\n",
       "      <td>0.999225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rj2u9e7x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/pearson</td><td>▁▄▅▆▆▆▇▇▇█▇██████████████</td></tr><tr><td>eval/runtime</td><td>▃▃▂▄▁▄▃▄▃▄▆▅▄▄▃▄▅▅█▄▇▇▃▃▃</td></tr><tr><td>eval/samples_per_second</td><td>▆▆▇▅█▅▆▅▆▅▃▄▅▅▅▅▄▄▁▅▂▂▆▆▆</td></tr><tr><td>eval/steps_per_second</td><td>▆▆▇▅█▅▆▅▆▅▃▄▅▅▆▅▄▄▁▅▂▂▆▆▆</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▆▇▁▂▃▄▅▆▇█▁▂▃▄▅▆▇█▁▂▃▄▅▆▇█▁▂▃▄▅▆▇█</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▆▇▁▂▃▄▅▆▇█▁▂▃▄▅▆▇█▁▂▃▄▅▆▇█▁▂▃▄▅▆▇█</td></tr><tr><td>train/grad_norm</td><td>█▅▅▄▄▄▄▃▄▃▃▂▂▁▂▁▁▂▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▃▂▁█▇▆▅▄▃▂▁█▇▆▅▄▃▂▁█▇▆▅▄▃▂▁█▇▆▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▇▅▅▄▃▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.01257</td></tr><tr><td>eval/pearson</td><td>0.99922</td></tr><tr><td>eval/runtime</td><td>22.5335</td></tr><tr><td>eval/samples_per_second</td><td>354.938</td></tr><tr><td>eval/steps_per_second</td><td>5.547</td></tr><tr><td>test/runtime</td><td>3.0399</td></tr><tr><td>test/samples_per_second</td><td>361.859</td></tr><tr><td>test/steps_per_second</td><td>5.921</td></tr><tr><td>total_flos</td><td>0.0</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>2500</td></tr><tr><td>train/grad_norm</td><td>0.48999</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0354</td></tr><tr><td>train_loss</td><td>0.04422</td></tr><tr><td>train_runtime</td><td>1468.7694</td></tr><tr><td>train_samples_per_second</td><td>108.911</td></tr><tr><td>train_steps_per_second</td><td>1.702</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">roberta-base</strong> at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka/runs/rj2u9e7x' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/yongruka/runs/rj2u9e7x</a><br/> View project at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/yongruka</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240924_015756-rj2u9e7x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rj2u9e7x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/moruka/wandb/run-20240924_040048-xvvf7a2u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka/runs/xvvf7a2u' target=\"_blank\">KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS</a></strong> to <a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/yongruka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka/runs/xvvf7a2u' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/yongruka/runs/xvvf7a2u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Fold : 1-  /  Now_model : [KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 24:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.245100</td>\n",
       "      <td>4.093363</td>\n",
       "      <td>0.938557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.460100</td>\n",
       "      <td>2.877481</td>\n",
       "      <td>0.965618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.353800</td>\n",
       "      <td>1.909438</td>\n",
       "      <td>0.977623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.822400</td>\n",
       "      <td>1.652497</td>\n",
       "      <td>0.981330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.643900</td>\n",
       "      <td>1.371942</td>\n",
       "      <td>0.983069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Fold : 2-  /  Now_model : [KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 24:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.576100</td>\n",
       "      <td>0.455560</td>\n",
       "      <td>0.985783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.164700</td>\n",
       "      <td>0.083068</td>\n",
       "      <td>0.989065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.030748</td>\n",
       "      <td>0.993247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.023502</td>\n",
       "      <td>0.995184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>0.027379</td>\n",
       "      <td>0.995906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Fold : 3-  /  Now_model : [KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 24:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.065908</td>\n",
       "      <td>0.993630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.995961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.018747</td>\n",
       "      <td>0.996164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.011201</td>\n",
       "      <td>0.998322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.016406</td>\n",
       "      <td>0.998571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Fold : 4-  /  Now_model : [KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1307' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1307/2500 12:24 < 11:20, 1.75 it/s, Epoch 2.61/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.996389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.018990</td>\n",
       "      <td>0.997765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = maketrain(args, training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/ephemeral/home/data/aug50000.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_csv(preds):\n",
    "    test_id = pd.read_csv('/data/ephemeral/home/data/test.csv')['id']\n",
    "    for name in preds:\n",
    "        d = pd.DataFrame({'id' : test_id, 'target' : preds[name].round(2)})\n",
    "        d.to_csv(f'{name}'.csv)\n",
    "        print(f'{name}'.csv)\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpreds\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_csv(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
