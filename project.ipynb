{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import torchmetrics\n",
    "from scipy.stats import pearsonr\n",
    "import evaluate\n",
    "import gc\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "random.seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, tokenizer, max_length, mode = 'train'):\n",
    "        self.inputs = inputs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.text_columns = ['sentence_1', 'sentence_2']\n",
    "        self.mode = mode\n",
    "    def cleaning_text(self, text):\n",
    "        cleaned_text = re.sub(r'[ㅋ-ㅎ]+', '', text)\n",
    "        cleaned_text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        t = self.inputs.iloc[idx] \n",
    "\n",
    "        text = '[SEP]'.join([t[col] for col in self.text_columns])\n",
    "        text = self.cleaning_text(text)\n",
    "        output = self.tokenizer(text,\n",
    "                                padding='max_length',\n",
    "                                max_length=self.max_length,\n",
    "                                truncation=True)\n",
    "\n",
    "        datas = torch.tensor(output['input_ids'], dtype = torch.long)\n",
    "        attn = torch.tensor(output['attention_mask'], dtype = torch.long)\n",
    "        # type_ids = torch.tensor(output['token_type_ids'], dtype = torch.long)\n",
    "        if self.mode == 'train':\n",
    "            labels = t['label']\n",
    "            output = {'input_ids' : datas,\n",
    "                      'attention_mask' : attn,\n",
    "                    #   'token_type_ids' : type_ids,\n",
    "                      'labels' : labels}\n",
    "            return output\n",
    "        else:\n",
    "            output = {'input_ids' : datas,\n",
    "                      'attention_mask' : attn,\n",
    "                    #   'token_type_ids' : type_ids\n",
    "                    }\n",
    "            return output\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model_preds):\n",
    "    preds, labels = model_preds\n",
    "    preds = torch.tensor(preds, dtype = torch.float32).squeeze(-1)\n",
    "    labels = torch.tensor(labels, dtype = torch.float32).squeeze(-1)\n",
    "    pear = torchmetrics.PearsonCorrCoef()\n",
    "    pearson = pear(preds, labels)\n",
    "    return {'pearson' : pearson.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(MyModel, self).__init__()\n",
    "        if model_name == \"snunlp/KR-ELECTRA-discriminator\":\n",
    "            print('found KR-ELECTRA')\n",
    "            self.model = ElectraModel.from_pretrained(\"snunlp/KR-ELECTRA-discriminator\")\n",
    "        else:\n",
    "            self.model = transformers.AutoModel.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "        # 첫 번째 Conv1D 레이어\n",
    "        self.Conv1 = nn.Conv1d(\n",
    "            in_channels=768,  # BERT의 출력 차원\n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        # 두 번째 Conv1D 레이어 (필요 시 추가)\n",
    "        self.Conv2 = nn.Conv1d(\n",
    "            in_channels=256,  # Conv1의 출력 차원\n",
    "            out_channels=128,  # Conv2의 출력 차원\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(128, 1)  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)  \n",
    "        self.batchnorm1 = nn.BatchNorm1d(256)  \n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)  \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size = 2)  \n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, \n",
    "                # token_type_ids,\n",
    "                  labels = None):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                            # token_type_ids=token_type_ids\n",
    "                            )\n",
    "        \n",
    "        output = output.last_hidden_state.permute(0, 2, 1)  \n",
    "\n",
    "        cnn_output = self.Conv1(output)  # Shape: (B, 256, L)\n",
    "        cnn_output = self.relu(cnn_output)\n",
    "        cnn_output = self.batchnorm1(cnn_output)  \n",
    "        cnn_output = self.dropout(cnn_output)\n",
    "        cnn_output = self.maxpool(cnn_output) # (B, 128, L/2)\n",
    "\n",
    "        cnn_output = self.Conv2(cnn_output)  # Shape: (B, 128, L/2)\n",
    "        cnn_output = self.relu(cnn_output)\n",
    "        cnn_output = self.batchnorm2(cnn_output) \n",
    "        cnn_output = self.dropout(cnn_output)\n",
    "        cnn_output = self.avg_pool(cnn_output)  #(B, 128, 1)\n",
    "\n",
    "        cnn_output = cnn_output.view(cnn_output.size(0), -1)  # Shape: (B, 128)\n",
    "        output = self.output_layer(cnn_output).squeeze(-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(output, labels.float())\n",
    "            return {'output' : output, 'loss' : loss}\n",
    "\n",
    "        else:  \n",
    "            return {'output' : output}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def maketrain(args,training_args):\n",
    "\n",
    "    model_list = args.model_list\n",
    "    max_length = args.max_length\n",
    "    k = args.kf\n",
    "    kf = KFold(n_splits = k, shuffle = True, random_state = 0)\n",
    "    data_routes = args.data_routes\n",
    "    preds = {}\n",
    "    test = pd.read_csv('/data/ephemeral/home/data/test.csv')\n",
    "    df = pd.DataFrame()\n",
    "    for route in data_routes:\n",
    "        df = pd.concat([df, pd.read_csv(route)])\n",
    "    df.reset_index(drop = True)\n",
    "    df = df[['sentence_1', 'sentence_2' ,'label']].dropna().reset_index(drop = True)\n",
    "\n",
    "\n",
    "\n",
    "    for model_name in model_list:\n",
    "        name = model_name.split('/')[-1]\n",
    "        model = MyModel(model_name)\n",
    "        training_args.output_dir = f\"./results/{name}\"\n",
    "        training_args.run_name = f'{name}'\n",
    "        wandb_run = wandb.init(project = \"yongruka\", name = f\"{name}\", reinit = True)\n",
    "        if model_name == \"snunlp/KR-ELECTRA-discriminator\":\n",
    "            print('found KR-ELECTRA')\n",
    "            tokenizer = ElectraTokenizer.from_pretrained(\"snunlp/KR-ELECTRA-discriminator\")\n",
    "        else:\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, trust_remote_code = True)\n",
    "        test_dataset = Dataset(test, tokenizer, max_length, mode = 'test')\n",
    "        data_collator = DataCollatorWithPadding(\n",
    "                tokenizer = tokenizer,\n",
    "                padding = True,\n",
    "                return_tensors = 'pt'\n",
    "            )\n",
    "        \n",
    "\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(df)):\n",
    "\n",
    "            print(f'-Fold : {fold+1}-  /  Now_model : [{name}]')\n",
    "\n",
    "            train_fold = df.iloc[train_index]\n",
    "            val_fold = df.iloc[val_index]\n",
    "            train_fold = Dataset(train_fold, tokenizer, max_length)\n",
    "            val_fold = Dataset(val_fold, tokenizer, max_length)\n",
    "\n",
    "            trainer = Trainer( \n",
    "                model = model,\n",
    "                tokenizer = tokenizer,\n",
    "                args = training_args,\n",
    "                train_dataset = train_fold,\n",
    "                eval_dataset = val_fold,\n",
    "                compute_metrics = compute_metrics,\n",
    "                data_collator = data_collator,\n",
    "                \n",
    "            )\n",
    "            for param in model.parameters():\n",
    "                if not param.is_contiguous():\n",
    "                    param.data = param.data.contiguous()\n",
    "\n",
    "        \n",
    "            trainer.train()\n",
    "        trainer.save_model(f'results/best_model_{name}')\n",
    "\n",
    "        pred = trainer.predict(test_dataset)\n",
    "        preds[name] = pred\n",
    "        gc.collect()\n",
    "        \n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:frutujde) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▄▂▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/pearson</td><td>▁▃▄▆▇▇▇▇█▇██</td></tr><tr><td>eval/runtime</td><td>▃▄▄▄█▁▇▆▃▇▅▃</td></tr><tr><td>eval/samples_per_second</td><td>▆▅▅▅▁█▂▃▆▂▄▆</td></tr><tr><td>eval/steps_per_second</td><td>▆▅▅▅▁█▂▃▆▂▄▆</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▄▅▅▆▇▇▁▂▃▃▄▅▅▆▇█▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▄▅▅▆▇▇▁▂▃▃▄▅▅▆▇█▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train/grad_norm</td><td>█▅▅▄▄▄▄▃▃▃▃▃▂▂▂▃▃▂▂▂▂▂▂▂▂▁▁▂▂▂▁▂▁▁▁▁▂▂▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▄▄▃▂▁█▇▆▆▅▄▃▃▂▁█▇▆▆▅▄▃▃▂▁█▇▆▅▅▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▅▄▄▄▃▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.0745</td></tr><tr><td>eval/pearson</td><td>0.99617</td></tr><tr><td>eval/runtime</td><td>25.9355</td></tr><tr><td>eval/samples_per_second</td><td>326.656</td></tr><tr><td>eval/steps_per_second</td><td>10.218</td></tr><tr><td>test/runtime</td><td>3.3077</td></tr><tr><td>test/samples_per_second</td><td>332.557</td></tr><tr><td>test/steps_per_second</td><td>10.581</td></tr><tr><td>total_flos</td><td>0.0</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>2385</td></tr><tr><td>train/grad_norm</td><td>2.71791</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1185</td></tr><tr><td>train_loss</td><td>0.1266</td></tr><tr><td>train_runtime</td><td>788.3082</td></tr><tr><td>train_samples_per_second</td><td>96.724</td></tr><tr><td>train_steps_per_second</td><td>3.025</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert-base</strong> at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka/runs/frutujde' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/yongruka/runs/frutujde</a><br/> View project at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/yongruka</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240924_205716-frutujde/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:frutujde). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/moruka/wandb/run-20240925_011425-r6vmbckk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka/runs/r6vmbckk' target=\"_blank\">clear-salad-64</a></strong> to <a href='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Yonruka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka/runs/r6vmbckk' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Yonruka/runs/r6vmbckk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka/runs/r6vmbckk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f3cfa62ffa0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!rm -rf /root/.cache/wandb\n",
    "!rm -rf /root/.config/wandb\n",
    "!rm -rf /root/.netrc\n",
    "os.environ[\"WANDB_API_KEY\"] = \"ea26fff0d932bc74bbfad9fd507b292c67444c02\"\n",
    "wandb.init(project=\"yonruka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [#'klue/roberta-small',\n",
    "            \"snunlp/KR-ELECTRA-discriminator\",\n",
    "            'Alibaba-NLP/gte-multilingual-base',\n",
    "            'klue/roberta-base',\n",
    "            #'snunlp/KR-SBERT-Medium-extended-klueNLItriplet_PARpair_QApair-klueSTS',\n",
    "            'snunlp/KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS',\n",
    "            'klue/bert-base',\n",
    "            #'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "\n",
    "            ]\n",
    "data_routes = ['/data/ephemeral/home/data/toast_processed_train.csv',]\n",
    "            #    '/data/ephemeral/home/data/dev.csv']\n",
    "            #    '/data/ephemeral/home/data/aug50000.csv']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_list', default = model_list, type = list)\n",
    "parser.add_argument('--batch_size', default = 32, type = int)\n",
    "parser.add_argument('--max_epoch', default = 3, type = int)\n",
    "parser.add_argument('--max_length', default = 160, type = int)\n",
    "parser.add_argument('--kf', default = 4, type = int)\n",
    "parser.add_argument('--data_routes', default = data_routes, type = list)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f\"./results/default\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = 'epoch',\n",
    "    per_device_train_batch_size = args.batch_size,\n",
    "    per_device_eval_batch_size = args.batch_size,\n",
    "    num_train_epochs = args.max_epoch,\n",
    "    weight_decay = 0.01,\n",
    "    logging_dir = './logs',\n",
    "    logging_steps = 30,\n",
    "    report_to = \"wandb\",  \n",
    "    run_name = \"default\",\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'pearson'\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found KR-ELECTRA\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:r6vmbckk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clear-salad-64</strong> at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka/runs/r6vmbckk' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Yonruka/runs/r6vmbckk</a><br/> View project at: <a href='https://wandb.ai/kimybjg2-boostcampaitech/Yonruka' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/Yonruka</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240925_011425-r6vmbckk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:r6vmbckk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/moruka/wandb/run-20240925_011606-4ad255ih</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka/runs/4ad255ih' target=\"_blank\">KR-ELECTRA-discriminator</a></strong> to <a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/yongruka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kimybjg2-boostcampaitech/yongruka/runs/4ad255ih' target=\"_blank\">https://wandb.ai/kimybjg2-boostcampaitech/yongruka/runs/4ad255ih</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found KR-ELECTRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Fold : 1-  /  Now_model : [KR-ELECTRA-discriminator]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='788' max='2385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 788/2385 03:45 < 07:37, 3.49 it/s, Epoch 0.99/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = maketrain(args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(preds):\n",
    "    test_id = pd.read_csv('/data/ephemeral/home/data/test.csv')['id']\n",
    "    for name in preds:\n",
    "        d = pd.DataFrame({'id' : test_id, 'target' : preds[name].predictions.round(2)})\n",
    "        d.to_csv(f'{name}.csv')\n",
    "        print(f'{name}.csv')\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KR-ELECTRA-discriminator.csv\n",
      "gte-multilingual-base.csv\n",
      "roberta-base.csv\n",
      "KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS.csv\n",
      "bert-base.csv\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "make_csv(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KR-ELECTRA-discriminator\n",
      "gte-multilingual-base\n",
      "roberta-base\n",
      "KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS\n",
      "bert-base\n"
     ]
    }
   ],
   "source": [
    "for name in preds:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
