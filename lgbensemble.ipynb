{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import transformers\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import safetensors\n",
    "from safetensors import safe_open\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, tokenizer, max_length, mode = 'train'):\n",
    "        self.inputs = inputs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.text_columns = ['sentence_1', 'sentence_2']\n",
    "        self.mode = mode\n",
    "    def cleaning_text(self, text):\n",
    "        cleaned_text = re.sub(r'[ㅋ-ㅎ]+', '', text)\n",
    "        cleaned_text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        t = self.inputs.iloc[idx] \n",
    "\n",
    "        text = '[SEP]'.join([t[col] for col in self.text_columns])\n",
    "        text = self.cleaning_text(text)\n",
    "        output = self.tokenizer(text,\n",
    "                                padding='max_length',\n",
    "                                max_length=self.max_length,\n",
    "                                truncation=True)\n",
    "\n",
    "        datas = torch.tensor(output['input_ids'], dtype = torch.long)\n",
    "        attn = torch.tensor(output['attention_mask'], dtype = torch.long)\n",
    "        # type_ids = torch.tensor(output['token_type_ids'], dtype = torch.long)\n",
    "        if self.mode == 'train':\n",
    "            labels = t['label']\n",
    "            output = {'input_ids' : datas,\n",
    "                      'attention_mask' : attn,\n",
    "                    #   'token_type_ids' : type_ids,\n",
    "                      'labels' : labels}\n",
    "            return output\n",
    "        else:\n",
    "            output = {'input_ids' : datas,\n",
    "                      'attention_mask' : attn,\n",
    "                    #   'token_type_ids' : type_ids\n",
    "                    }\n",
    "            return output\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(MyModel, self).__init__()\n",
    "        if model_name == \"snunlp/KR-ELECTRA-discriminator\":\n",
    "            print('found KR-ELECTRA')\n",
    "            self.model = ElectraModel.from_pretrained(\"snunlp/KR-ELECTRA-discriminator\")\n",
    "        else:\n",
    "            self.model = transformers.AutoModel.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "        # 첫 번째 Conv1D 레이어\n",
    "        self.Conv1 = nn.Conv1d(\n",
    "            in_channels=768,  # BERT의 출력 차원\n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        # 두 번째 Conv1D 레이어 (필요 시 추가)\n",
    "        self.Conv2 = nn.Conv1d(\n",
    "            in_channels=256,  # Conv1의 출력 차원\n",
    "            out_channels=128,  # Conv2의 출력 차원\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(128, 1)  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)  \n",
    "        self.batchnorm1 = nn.BatchNorm1d(256)  \n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)  \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size = 2)  \n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, \n",
    "                # token_type_ids,\n",
    "                  labels = None):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                            # token_type_ids=token_type_ids\n",
    "                            )\n",
    "        \n",
    "        output = output.last_hidden_state.permute(0, 2, 1)  \n",
    "\n",
    "        cnn_output = self.Conv1(output)  # Shape: (B, 256, L)\n",
    "        cnn_output = self.relu(cnn_output)\n",
    "        cnn_output = self.batchnorm1(cnn_output)  \n",
    "        cnn_output = self.dropout(cnn_output)\n",
    "        cnn_output = self.maxpool(cnn_output) # (B, 128, L/2)\n",
    "\n",
    "        cnn_output = self.Conv2(cnn_output)  # Shape: (B, 128, L/2)\n",
    "        cnn_output = self.relu(cnn_output)\n",
    "        cnn_output = self.batchnorm2(cnn_output) \n",
    "        cnn_output = self.dropout(cnn_output)\n",
    "        cnn_output = self.avg_pool(cnn_output)  #(B, 128, 1)\n",
    "\n",
    "        cnn_output = cnn_output.view(cnn_output.size(0), -1)  # Shape: (B, 128)\n",
    "        output = self.output_layer(cnn_output).squeeze(-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(output, labels.float())\n",
    "            return {'output' : output, 'loss' : loss}\n",
    "\n",
    "        else:  \n",
    "            return {'output' : output}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    model_name = name.split('/')[-1]\n",
    "    model = MyModel(name)\n",
    "    if name == 'snunlp/KR-ELECTR-discriminator':\n",
    "        tokenizer = ElectraTokenizer.from_pretrained(\"snunlp/KR-ELECTRA-discriminator\")\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    tensors = {}\n",
    "    model_path = f\"./results/best_model_{model_name}\"\n",
    "    with safe_open(model_path + \"/model.safetensors\", framework=\"pt\", device=0) as f:\n",
    "        for k in f.keys():\n",
    "            tensors[k] = f.get_tensor(k)\n",
    "    model.load_state_dict(tensors)\n",
    "    print(f'matched all parameters.[{model_name}]')\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model_list, mode = 'train'):\n",
    "    if mode == 'train':\n",
    "        data = pd.read_csv('/data/ephemeral/home/data/dev.csv')\n",
    "        data = data[['sentence_1', 'sentence_2', 'label']].dropna()\n",
    "    else:\n",
    "        data = pd.read_csv('/data/ephemeral/home/data/test.csv')\n",
    "        id = data['id']\n",
    "    preds = {}\n",
    "    for name in model_list:\n",
    "        model, tokenizer = load_model(name)\n",
    "        if mode == 'train':\n",
    "            dataset = Dataset(data, tokenizer, 160)\n",
    "            data_collator = DataCollatorWithPadding(\n",
    "                    tokenizer = tokenizer,\n",
    "                    padding = True,\n",
    "                    return_tensors = 'pt'\n",
    "                )\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, collate_fn = data_collator, batch_size = 32)\n",
    "        else:\n",
    "            dataset = Dataset(data, tokenizer, 160, mode = 'test')\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size = 32)\n",
    "\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        all_outputs = np.array([])\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                # token_type_ids = batch['token_type_ids'].to(device) \n",
    "\n",
    "                output = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                # token_type_ids=token_type_ids\n",
    "                                )['output']\n",
    "                all_outputs = np.hstack((all_outputs,output.cpu().numpy()))\n",
    "            preds[name] = all_outputs\n",
    "            \n",
    "    return pd.DataFrame(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lgb_train(preds, label):\n",
    "    lgb_dataset = lgb.Dataset(preds, label = label)\n",
    "    lgb_params = {\n",
    "         'objective': 'regression',\n",
    "         'metric': 'mse',\n",
    "         'boosting_type': 'gbdt',\n",
    "         'learning_rate': 0.05,\n",
    "         'num_leaves': 31,\n",
    "         'max_depth': -1,\n",
    "         'min_data_in_leaf': 20,\n",
    "         'feature_fraction': 0.8,\n",
    "         'bagging_fraction': 0.8,\n",
    "         'bagging_freq': 1,\n",
    "         'verbose': -1,\n",
    "         'random_state': 42\n",
    "         }\n",
    "    lgb_model = lgb.train(lgb_params, lgb_dataset)\n",
    "    return lgb_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found KR-ELECTRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched all parameters.[KR-ELECTRA-discriminator]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00,  9.70it/s]\n",
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched all parameters.[gte-multilingual-base]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:02<00:00,  8.25it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched all parameters.[roberta-base]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched all parameters.[KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00, 10.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched all parameters.[bert-base]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00, 10.77it/s]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/data/ephemeral/home/data/dev.csv')\n",
    "train = train[['sentence_1', 'sentence_2', 'label']].dropna()\n",
    "labels = train['label']\n",
    "model_list = [#'klue/roberta-small',\n",
    "            \"snunlp/KR-ELECTRA-discriminator\",\n",
    "            'Alibaba-NLP/gte-multilingual-base',\n",
    "            'klue/roberta-base',\n",
    "            #'snunlp/KR-SBERT-Medium-extended-klueNLItriplet_PARpair_QApair-klueSTS',\n",
    "            'snunlp/KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS',\n",
    "            'klue/bert-base',\n",
    "            #'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "\n",
    "            ]\n",
    "\n",
    "preds = get_predictions(model_list)\n",
    "lgb_model = lgb_train(preds, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found KR-ELECTRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched all parameters.[KR-ELECTRA-discriminator]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:03<00:00, 11.20it/s]\n",
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched all parameters.[gte-multilingual-base]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:04<00:00,  8.31it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched all parameters.[roberta-base]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:03<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched all parameters.[KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:03<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched all parameters.[bert-base]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:03<00:00, 10.88it/s]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('/data/ephemeral/home/data/test.csv')\n",
    "test_dataset = get_predictions(model_list, mode = 'test')\n",
    "predictions = lgb_model.predict(test_dataset)\n",
    "pd.DataFrame({'id' : test['id'], 'target' : predictions.round(2)}).to_csv('lgb_ensemble.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.9215427 , 4.09365326, 1.56452448, ..., 3.84821289, 3.01351541,\n",
       "       4.66127263])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
