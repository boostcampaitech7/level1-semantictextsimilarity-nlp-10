{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-24 04:40:18.077018: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-24 04:40:18.083139: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-24 04:40:18.095903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-24 04:40:18.120441: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-24 04:40:18.129112: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-24 04:40:18.149013: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-24 04:40:19.838060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import transformers\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import safetensors\n",
    "from safetensors import safe_open\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, tokenizer, max_length, mode = 'train'):\n",
    "        self.inputs = inputs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.text_columns = ['sentence_1', 'sentence_2']\n",
    "        self.mode = mode\n",
    "    def cleaning_text(self, text):\n",
    "        cleaned_text = re.sub(r'[ㅋ-ㅎ]+', '', text)\n",
    "        cleaned_text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        t = self.inputs.iloc[idx] \n",
    "\n",
    "        text = '[SEP]'.join([t[col] for col in self.text_columns])\n",
    "        text = self.cleaning_text(text)\n",
    "        output = self.tokenizer(text,\n",
    "                                padding='max_length',\n",
    "                                max_length=self.max_length,\n",
    "                                truncation=True)\n",
    "\n",
    "        datas = torch.tensor(output['input_ids'], dtype = torch.long)\n",
    "        attn = torch.tensor(output['attention_mask'], dtype = torch.long)\n",
    "        type_ids = torch.tensor(output['token_type_ids'], dtype = torch.long)\n",
    "        if self.mode == 'train':\n",
    "            labels = t['label']\n",
    "            output = {'input_ids' : datas,\n",
    "                      'attention_mask' : attn,\n",
    "                      'token_type_ids' : type_ids,\n",
    "                      'labels' : labels}\n",
    "            return output\n",
    "        else:\n",
    "            output = {'input_ids' : datas,\n",
    "                      'attention_mask' : attn,\n",
    "                      'token_type_ids' : type_ids}\n",
    "            return output\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = transformers.AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # 첫 번째 Conv1D 레이어\n",
    "        self.Conv1 = nn.Conv1d(\n",
    "            in_channels=768,  # BERT의 출력 차원\n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        # 두 번째 Conv1D 레이어 (필요 시 추가)\n",
    "        self.Conv2 = nn.Conv1d(\n",
    "            in_channels=256,  # Conv1의 출력 차원\n",
    "            out_channels=128,  # Conv2의 출력 차원\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(128, 1)  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)  \n",
    "        self.batchnorm1 = nn.BatchNorm1d(256)  \n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)  \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size = 2)  \n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels = None):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        output = output.last_hidden_state.permute(0, 2, 1)  \n",
    "\n",
    "        cnn_output = self.Conv1(output)  # Shape: (B, 256, L)\n",
    "        cnn_output = self.relu(cnn_output)\n",
    "        cnn_output = self.batchnorm1(cnn_output)  \n",
    "        cnn_output = self.maxpool(cnn_output) # (B, 128, L/2)\n",
    "\n",
    "        cnn_output = self.Conv2(cnn_output)  # Shape: (B, 128, L/2)\n",
    "        cnn_output = self.relu(cnn_output)\n",
    "        cnn_output = self.batchnorm2(cnn_output) \n",
    "        cnn_output = self.avg_pool(cnn_output)  #(B, 128, 1)\n",
    "\n",
    "        cnn_output = cnn_output.view(cnn_output.size(0), -1)  # Shape: (B, 128)\n",
    "        output = self.output_layer(cnn_output).squeeze(-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(output, labels.float())\n",
    "            return {'output' : output, 'loss' : loss}\n",
    "\n",
    "        else:  \n",
    "            return {'output' : output}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    model_name = name.split('/')[-1]\n",
    "    model = MyModel(name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    tensors = {}\n",
    "    model_path = f\"./results/best_model_{model_name}\"\n",
    "    with safe_open(model_path + \"/model.safetensors\", framework=\"pt\", device=0) as f:\n",
    "        for k in f.keys():\n",
    "            tensors[k] = f.get_tensor(k)\n",
    "    model.load_state_dict(tensors)\n",
    "    print(f'matched all parameters.[{model_name}]')\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model_list, mode = 'train'):\n",
    "    if mode == 'train':\n",
    "        data = pd.read_csv('/data/ephemeral/home/data/lgbdata10000.csv')\n",
    "        data = data[['sentence_1', 'sentence_2', 'label']].dropna()\n",
    "    else:\n",
    "        data = pd.read_csv('/data/ephemeral/home/data/test.csv')\n",
    "        id = data['id']\n",
    "    preds = {}\n",
    "    for name in model_list:\n",
    "        model, tokenizer = load_model(name)\n",
    "        if mode == 'train':\n",
    "            dataset = Dataset(data, tokenizer, 160)\n",
    "            data_collator = DataCollatorWithPadding(\n",
    "                    tokenizer = tokenizer,\n",
    "                    padding = True,\n",
    "                    return_tensors = 'pt'\n",
    "                )\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, collate_fn = data_collator, batch_size = 32)\n",
    "        else:\n",
    "            dataset = Dataset(data, tokenizer, 160, mode = 'test')\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size = 32)\n",
    "\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        all_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device) \n",
    "\n",
    "                output = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                all_outputs.append(output)\n",
    "            preds[name] = all_outputs.cpu().numpy()\n",
    "            \n",
    "    return pd.DataFrame(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/ephemeral/home/data/lgbdata10000.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m train \u001b[38;5;241m=\u001b[39m train[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m      3\u001b[0m label \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/data/ephemeral/home/data/lgbdata10000.csv')\n",
    "train = train[['sentence_1', 'sentence_2', 'label']].dropna()\n",
    "label = train['label']\n",
    "model_list = [#'klue/roberta-small',\n",
    "            'klue/roberta-base',\n",
    "            #'snunlp/KR-SBERT-Medium-extended-klueNLItriplet_PARpair_QApair-klueSTS',\n",
    "            'snunlp/KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS',\n",
    "            'klue/bert-base',\n",
    "            #'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "            #  'Alibaba-NLP/gte-multilingual-base',\n",
    "            ]\n",
    "\n",
    "preds = get_predictions(model_list)\n",
    "lgb_dataset = lgb.Dataset(preds, label = label)\n",
    "lgb_params = {\n",
    "    'objective' : 'regression',\n",
    "    'metric' : 'mse',\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'learning_rate' : 0.05,\n",
    "    'num_leaves' : 31,\n",
    "    'max_depth' : -1,\n",
    "    'min_data_in_leaf' : 20,\n",
    "    'feature_fraction' : 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq' : 1,\n",
    "    'verbose' : -1,\n",
    "    'n_estimators' : 100,\n",
    "    'random_state' : 42\n",
    "}\n",
    "lgb_model = lgb.train(lgb_params, lgb_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/data/ephemeral/home/data/test.csv')\n",
    "test_dataset = get_predictions(model_list, mode = 'test')\n",
    "predictions = lgb_model.predict(test_dataset)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
