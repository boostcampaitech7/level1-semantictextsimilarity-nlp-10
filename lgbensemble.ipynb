{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import transformers\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import safetensors\n",
    "from safetensors import safe_open\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, tokenizer, max_length, mode = 'train'):\n",
    "        self.inputs = inputs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.text_columns = ['sentence_1', 'sentence_2']\n",
    "        self.mode = mode\n",
    "    def cleaning_text(self, text):\n",
    "        cleaned_text = re.sub(r'[ㅋ-ㅎ]+', '', text)\n",
    "        cleaned_text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        t = self.inputs.iloc[idx] \n",
    "\n",
    "        text = '[SEP]'.join([t[col] for col in self.text_columns])\n",
    "        text = self.cleaning_text(text)\n",
    "        output = self.tokenizer(text,\n",
    "                                padding='max_length',\n",
    "                                max_length=self.max_length,\n",
    "                                truncation=True)\n",
    "\n",
    "        datas = torch.tensor(output['input_ids'], dtype = torch.long)\n",
    "        attn = torch.tensor(output['attention_mask'], dtype = torch.long)\n",
    "        type_ids = torch.tensor(output['token_type_ids'], dtype = torch.long)\n",
    "        if self.mode == 'train':\n",
    "            labels = t['label']\n",
    "            output = {'input_ids' : datas,\n",
    "                      'attention_mask' : attn,\n",
    "                      'token_type_ids' : type_ids,\n",
    "                      'labels' : labels}\n",
    "            return output\n",
    "        else:\n",
    "            output = {'input_ids' : datas,\n",
    "                      'attention_mask' : attn,\n",
    "                      'token_type_ids' : type_ids}\n",
    "            return output\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = transformers.AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # 첫 번째 Conv1D 레이어\n",
    "        self.Conv1 = nn.Conv1d(\n",
    "            in_channels=768,  # BERT의 출력 차원\n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        # 두 번째 Conv1D 레이어 (필요 시 추가)\n",
    "        self.Conv2 = nn.Conv1d(\n",
    "            in_channels=256,  # Conv1의 출력 차원\n",
    "            out_channels=128,  # Conv2의 출력 차원\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(128, 1)  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)  \n",
    "        self.batchnorm1 = nn.BatchNorm1d(256)  \n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)  \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size = 2)  \n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels = None):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        output = output.last_hidden_state.permute(0, 2, 1)  \n",
    "\n",
    "        cnn_output = self.Conv1(output)  # Shape: (B, 256, L)\n",
    "        cnn_output = self.relu(cnn_output)\n",
    "        cnn_output = self.batchnorm1(cnn_output)  \n",
    "        cnn_output = self.maxpool(cnn_output) # (B, 128, L/2)\n",
    "\n",
    "        cnn_output = self.Conv2(cnn_output)  # Shape: (B, 128, L/2)\n",
    "        cnn_output = self.relu(cnn_output)\n",
    "        cnn_output = self.batchnorm2(cnn_output) \n",
    "        cnn_output = self.avg_pool(cnn_output)  #(B, 128, 1)\n",
    "\n",
    "        cnn_output = cnn_output.view(cnn_output.size(0), -1)  # Shape: (B, 128)\n",
    "        output = self.output_layer(cnn_output).squeeze(-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(output, labels.float())\n",
    "            return {'output' : output, 'loss' : loss}\n",
    "\n",
    "        else:  \n",
    "            return {'output' : output}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    model_name = name.split('/')[-1]\n",
    "    model = MyModel(name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    tensors = {}\n",
    "    model_path = f\"./results/best_model_{model_name}\"\n",
    "    with safe_open(model_path + \"/model.safetensors\", framework=\"pt\", device=0) as f:\n",
    "        for k in f.keys():\n",
    "            tensors[k] = f.get_tensor(k)\n",
    "    model.load_state_dict(tensors)\n",
    "    print(f'matched all parameters.[{model_name}]')\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model_list, mode = 'train'):\n",
    "    if mode == 'train':\n",
    "        data = pd.read_csv('/data/ephemeral/home/data/lgbdata10000.csv')\n",
    "        data = data[['sentence_1', 'sentence_2', 'label']].dropna()\n",
    "    else:\n",
    "        data = pd.read_csv('/data/ephemeral/home/data/test.csv')\n",
    "        id = data['id']\n",
    "    preds = {}\n",
    "    for name in model_list:\n",
    "        model, tokenizer = load_model(name)\n",
    "        if mode == 'train':\n",
    "            dataset = Dataset(data, tokenizer, 160)\n",
    "            data_collator = DataCollatorWithPadding(\n",
    "                    tokenizer = tokenizer,\n",
    "                    padding = True,\n",
    "                    return_tensors = 'pt'\n",
    "                )\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, collate_fn = data_collator, batch_size = 32)\n",
    "        else:\n",
    "            dataset = Dataset(data, tokenizer, 160, mode = 'test')\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size = 32)\n",
    "\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        all_outputs = np.array([])\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device) \n",
    "\n",
    "                output = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)['output']\n",
    "                all_outputs = np.hstack((all_outputs,output.cpu().numpy()))\n",
    "            preds[name] = all_outputs\n",
    "            \n",
    "    return pd.DataFrame(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lgb_train(preds, label):\n",
    "    lgb_dataset = lgb.Dataset(preds, label = label)\n",
    "    lgb_params = {\n",
    "         'objective': 'regression',\n",
    "         'metric': 'mse',\n",
    "         'boosting_type': 'gbdt',\n",
    "         'learning_rate': 0.05,\n",
    "         'num_leaves': 31,\n",
    "         'max_depth': -1,\n",
    "         'min_data_in_leaf': 20,\n",
    "         'feature_fraction': 0.8,\n",
    "         'bagging_fraction': 0.8,\n",
    "         'bagging_freq': 1,\n",
    "         'verbose': -1,\n",
    "         'random_state': 42\n",
    "         }\n",
    "    lgb_model = lgb.train(lgb_params, lgb_dataset)\n",
    "    return lgb_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched all parameters.[roberta-base]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/313 [00:00<02:11,  2.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m model_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;66;03m#'klue/roberta-small',\u001b[39;00m\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mklue/roberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;66;03m#'snunlp/KR-SBERT-Medium-extended-klueNLItriplet_PARpair_QApair-klueSTS',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;66;03m#  'Alibaba-NLP/gte-multilingual-base',\u001b[39;00m\n\u001b[1;32m     11\u001b[0m             ]\n\u001b[0;32m---> 13\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m lgb_model \u001b[38;5;241m=\u001b[39m lgb_train(preds, labels)\n",
      "Cell \u001b[0;32mIn[56], line 33\u001b[0m, in \u001b[0;36mget_predictions\u001b[0;34m(model_list, mode)\u001b[0m\n\u001b[1;32m     30\u001b[0m             token_type_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m     32\u001b[0m             output \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 33\u001b[0m             all_outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((all_outputs,\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[1;32m     34\u001b[0m         preds[name] \u001b[38;5;241m=\u001b[39m all_outputs\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(preds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/data/ephemeral/home/moruka/lgbdata5000.csv')\n",
    "train = train[['sentence_1', 'sentence_2', 'label']].dropna()\n",
    "labels = train['label']\n",
    "model_list = [#'klue/roberta-small',\n",
    "            'klue/roberta-base',\n",
    "            #'snunlp/KR-SBERT-Medium-extended-klueNLItriplet_PARpair_QApair-klueSTS',\n",
    "            'snunlp/KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS',\n",
    "            'klue/bert-base',\n",
    "            #'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "            #  'Alibaba-NLP/gte-multilingual-base',\n",
    "            ]\n",
    "\n",
    "preds = get_predictions(model_list)\n",
    "lgb_model = lgb_train(preds, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('/data/ephemeral/home/data/test.csv')\n",
    "test_dataset = get_predictions(model_list, mode = 'test')\n",
    "predictions = lgb_model.predict(test_dataset)\n",
    "pd.DataFrame({'id' : test['id'], 'target' : predictions.round(2)}).to_csv('lgb_ensemble.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
